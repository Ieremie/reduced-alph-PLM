''' Adapted from https://github.com/tbepler/prose'''

from __future__ import print_function, division
import torch
import torch.nn as nn
from torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence, pack_sequence
import torch.nn.functional as F


class SkipLSTM(nn.Module):
    def __init__(self, nin, nout, hidden_dim, num_layers, dropout=0, bidirectional=True):
        super(SkipLSTM, self).__init__()

        self.nin = nin
        self.nout = nout

        # use this instead of 1-hot encoding to allow projecting AA embeddings for plots
        # the model always has the ability to use 1-hot encoding
        # used in UNIREP paper
        self.aa_embedding = nn.Embedding(nin, nin)

        self.dropout = nn.Dropout(p=dropout)
        self.layers = nn.ModuleList()

        dim = nin
        bidi_dim = 2 if bidirectional else 1
        for i in range(num_layers):
            f = nn.LSTM(dim, hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirectional)
            self.layers.append(f)
            dim = bidi_dim * hidden_dim

        n = bidi_dim * hidden_dim * num_layers + nin

        self.proj = nn.Linear(n, nout)
        self.cloze = nn.Linear(n, nin)

    def forward(self, seqs, seqs_lengths, apply_proj=True):

        # flatten parameters that have been scattered during multi gpu training
        for lstm_layer in self.layers:
            lstm_layer.flatten_parameters()

        # Required for multi GPU training: padding to max seq length on each GPU to match the output
        total_length = seqs.size(1)

        # project encoded sequences to embedding space
        #seqs_one_hot = F.one_hot(seqs, num_classes=self.nin).to(torch.float)
        seqs_one_hot = self.aa_embedding(seqs)

        packed_input = pack_padded_sequence(seqs_one_hot, seqs_lengths.to('cpu'),
                                            batch_first=True, enforce_sorted=False)

        aggregate_output = [packed_input]
        model_input = packed_input

        for lstm_layer in self.layers:
            packed_output, _ = lstm_layer(model_input)
            aggregate_output.append(packed_output)
            model_input = packed_output

        # concatenating the AA representations generated by each layer plus the simple 1-hot
        packed_output = torch.cat([z.data for z in aggregate_output], dim=1)
        z = self.proj(packed_output) if apply_proj else packed_output
        z = PackedSequence(z, packed_input.batch_sizes, packed_input.sorted_indices, packed_input.unsorted_indices)

        # multi GPU requirement
        # unpacking to pad sequences to aggregate information from all GPUs
        padded_output, _ = pad_packed_sequence(z, batch_first=True, total_length=total_length)

        return padded_output
